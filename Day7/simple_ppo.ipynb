{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "simple_ppo.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kim-Ju-won/HUFS-Winter-AI-Reinforcement-Learning/blob/main/Day7/simple_ppo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir PPO"
      ],
      "metadata": {
        "id": "6rLu467vAbkx"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch as T\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions.categorical import Categorical\n",
        "\n",
        "class PPOMemory:\n",
        "    def __init__(self, batch_size):\n",
        "        self.states = []\n",
        "        self.probs = []\n",
        "        self.vals = []\n",
        "        self.actions = []\n",
        "        self.rewards = []\n",
        "        self.dones = []\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def generate_batches(self):\n",
        "        n_states = len(self.states)\n",
        "        batch_start = np.arange(0, n_states, self.batch_size)\n",
        "        indices = np.arange(n_states, dtype=np.int64)\n",
        "        np.random.shuffle(indices)\n",
        "        batches = [indices[i:i+self.batch_size] for i in batch_start]\n",
        "\n",
        "        return np.array(self.states),\\\n",
        "                np.array(self.actions),\\\n",
        "                np.array(self.probs),\\\n",
        "                np.array(self.vals),\\\n",
        "                np.array(self.rewards),\\\n",
        "                np.array(self.dones),\\\n",
        "                batches\n",
        "\n",
        "    def store_memory(self, state, action, probs, vals, reward, done):\n",
        "        self.states.append(state)\n",
        "        self.actions.append(action)\n",
        "        self.probs.append(probs)\n",
        "        self.vals.append(vals)\n",
        "        self.rewards.append(reward)\n",
        "        self.dones.append(done)\n",
        "\n",
        "    def clear_memory(self):\n",
        "        self.states = []\n",
        "        self.probs = []\n",
        "        self.actions = []\n",
        "        self.rewards = []\n",
        "        self.dones = []\n",
        "        self.vals = []\n",
        "\n",
        "class ActorNetwork(nn.Module):\n",
        "    def __init__(self, n_actions, input_dims, alpha,\n",
        "            fc1_dims=256, fc2_dims=256, chkpt_dir='PPO'):\n",
        "        super(ActorNetwork, self).__init__()\n",
        "\n",
        "        self.checkpoint_file = os.path.join(chkpt_dir, 'actor_torch_ppo')\n",
        "        self.actor = nn.Sequential(\n",
        "                nn.Linear(*input_dims, fc1_dims),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(fc1_dims, fc2_dims),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(fc2_dims, n_actions),\n",
        "                nn.Softmax(dim=-1)\n",
        "        )\n",
        "\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=alpha)\n",
        "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
        "        self.to(self.device)\n",
        "\n",
        "    def forward(self, state):\n",
        "        dist = self.actor(state)\n",
        "        dist = Categorical(dist)\n",
        "        \n",
        "        return dist\n",
        "\n",
        "    def save_checkpoint(self):\n",
        "        T.save(self.state_dict(), self.checkpoint_file)\n",
        "\n",
        "    def load_checkpoint(self):\n",
        "        self.load_state_dict(T.load(self.checkpoint_file))\n",
        "\n",
        "class CriticNetwork(nn.Module):\n",
        "    def __init__(self, input_dims, alpha, fc1_dims=256, fc2_dims=256,\n",
        "            chkpt_dir='PPO'):\n",
        "        super(CriticNetwork, self).__init__()\n",
        "\n",
        "        self.checkpoint_file = os.path.join(chkpt_dir, 'critic_torch_ppo')\n",
        "        self.critic = nn.Sequential(\n",
        "                nn.Linear(*input_dims, fc1_dims),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(fc1_dims, fc2_dims),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(fc2_dims, 1)\n",
        "        )\n",
        "\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=alpha)\n",
        "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
        "        self.to(self.device)\n",
        "\n",
        "    def forward(self, state):\n",
        "        value = self.critic(state)\n",
        "\n",
        "        return value\n",
        "\n",
        "    def save_checkpoint(self):\n",
        "        T.save(self.state_dict(), self.checkpoint_file)\n",
        "\n",
        "    def load_checkpoint(self):\n",
        "        self.load_state_dict(T.load(self.checkpoint_file))\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, n_actions, input_dims, gamma=0.99, alpha=0.0003, gae_lambda=0.95,\n",
        "            policy_clip=0.2, batch_size=64, n_epochs=10):\n",
        "        self.gamma = gamma\n",
        "        self.policy_clip = policy_clip\n",
        "        self.n_epochs = n_epochs\n",
        "        self.gae_lambda = gae_lambda\n",
        "\n",
        "        self.actor = ActorNetwork(n_actions, input_dims, alpha)\n",
        "        self.critic = CriticNetwork(input_dims, alpha)\n",
        "        self.memory = PPOMemory(batch_size)\n",
        "       \n",
        "    def remember(self, state, action, probs, vals, reward, done):\n",
        "        self.memory.store_memory(state, action, probs, vals, reward, done)\n",
        "\n",
        "    def save_models(self):\n",
        "        print('... saving models ...')\n",
        "        self.actor.save_checkpoint()\n",
        "        self.critic.save_checkpoint()\n",
        "\n",
        "    def load_models(self):\n",
        "        print('... loading models ...')\n",
        "        self.actor.load_checkpoint()\n",
        "        self.critic.load_checkpoint()\n",
        "\n",
        "    def choose_action(self, observation):\n",
        "        state = T.tensor([observation], dtype=T.float).to(self.actor.device)\n",
        "\n",
        "        dist = self.actor(state)\n",
        "        value = self.critic(state)\n",
        "        action = dist.sample()\n",
        "\n",
        "        probs = T.squeeze(dist.log_prob(action)).item()\n",
        "        action = T.squeeze(action).item()\n",
        "        value = T.squeeze(value).item()\n",
        "\n",
        "        return action, probs, value\n",
        "\n",
        "    def learn(self):\n",
        "        for _ in range(self.n_epochs):\n",
        "            state_arr, action_arr, old_prob_arr, vals_arr,\\\n",
        "            reward_arr, dones_arr, batches = \\\n",
        "                    self.memory.generate_batches()\n",
        "\n",
        "            values = vals_arr\n",
        "            advantage = np.zeros(len(reward_arr), dtype=np.float32)\n",
        "\n",
        "            for t in range(len(reward_arr)-1):\n",
        "                discount = 1\n",
        "                a_t = 0\n",
        "                for k in range(t, len(reward_arr)-1):\n",
        "                    a_t += discount*(reward_arr[k] + self.gamma*values[k+1]*\\\n",
        "                            (1-int(dones_arr[k])) - values[k])\n",
        "                    discount *= self.gamma*self.gae_lambda\n",
        "                advantage[t] = a_t\n",
        "            advantage = T.tensor(advantage).to(self.actor.device)\n",
        "\n",
        "            values = T.tensor(values).to(self.actor.device)\n",
        "            for batch in batches:\n",
        "                states = T.tensor(state_arr[batch], dtype=T.float).to(self.actor.device)\n",
        "                old_probs = T.tensor(old_prob_arr[batch]).to(self.actor.device)\n",
        "                actions = T.tensor(action_arr[batch]).to(self.actor.device)\n",
        "\n",
        "                dist = self.actor(states)\n",
        "                critic_value = self.critic(states)\n",
        "\n",
        "                critic_value = T.squeeze(critic_value)\n",
        "\n",
        "                new_probs = dist.log_prob(actions)\n",
        "                prob_ratio = new_probs.exp() / old_probs.exp()\n",
        "                #prob_ratio = (new_probs - old_probs).exp()\n",
        "                weighted_probs = advantage[batch] * prob_ratio\n",
        "                weighted_clipped_probs = T.clamp(prob_ratio, 1-self.policy_clip,\n",
        "                        1+self.policy_clip)*advantage[batch]\n",
        "                actor_loss = -T.min(weighted_probs, weighted_clipped_probs).mean()\n",
        "\n",
        "                returns = advantage[batch] + values[batch]\n",
        "                critic_loss = (returns-critic_value)**2\n",
        "                critic_loss = critic_loss.mean()\n",
        "\n",
        "                total_loss = actor_loss + 0.5*critic_loss\n",
        "                self.actor.optimizer.zero_grad()\n",
        "                self.critic.optimizer.zero_grad()\n",
        "                total_loss.backward()\n",
        "                self.actor.optimizer.step()\n",
        "                self.critic.optimizer.step()\n",
        "\n",
        "        self.memory.clear_memory()               \n",
        "\n"
      ],
      "metadata": {
        "id": "E16zWxaLAk6I"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_learning_curve(x, scores, figure_file):\n",
        "    running_avg = np.zeros(len(scores))\n",
        "    for i in range(len(running_avg)):\n",
        "        running_avg[i] = np.mean(scores[max(0, i-100):(i+1)])\n",
        "    plt.plot(x, running_avg)\n",
        "    plt.title('Running average of previous 100 scores')\n",
        "    plt.savefig(figure_file)"
      ],
      "metadata": {
        "id": "FjiGDZ7sAtWy"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "env = gym.make('CartPole-v0')\n",
        "N = 20\n",
        "batch_size = 5\n",
        "n_epochs = 4\n",
        "alpha = 0.0003\n",
        "agent = Agent(n_actions=env.action_space.n, batch_size=batch_size, \n",
        "                alpha=alpha, n_epochs=n_epochs, \n",
        "                input_dims=env.observation_space.shape)\n",
        "n_games = 300\n",
        "\n",
        "figure_file = 'PPO/cartpole.png'\n",
        "\n",
        "best_score = env.reward_range[0]\n",
        "score_history = []\n",
        "\n",
        "learn_iters = 0\n",
        "avg_score = 0\n",
        "n_steps = 0\n",
        "\n",
        "for i in range(n_games):\n",
        "    observation = env.reset()\n",
        "    done = False\n",
        "    score = 0\n",
        "    while not done:\n",
        "        action, prob, val = agent.choose_action(observation)\n",
        "        observation_, reward, done, info = env.step(action)\n",
        "        n_steps += 1\n",
        "        score += reward\n",
        "        agent.remember(observation, action, prob, val, reward, done)\n",
        "        if n_steps % N == 0:\n",
        "            agent.learn()\n",
        "            learn_iters += 1\n",
        "        observation = observation_\n",
        "    score_history.append(score)\n",
        "    avg_score = np.mean(score_history[-100:])\n",
        "\n",
        "    if avg_score > best_score:\n",
        "        best_score = avg_score\n",
        "        agent.save_models()\n",
        "\n",
        "    print('episode', i, 'score %.1f' % score, 'avg score %.1f' % avg_score,\n",
        "            'time_steps', n_steps, 'learning_steps', learn_iters)\n",
        "x = [i+1 for i in range(len(score_history))]\n",
        "plot_learning_curve(x, score_history, figure_file)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ptfy20jLA1ED",
        "outputId": "b0d6e853-659d-4111-f663-615675c791aa"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:130: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "... saving models ...\n",
            "episode 0 score 23.0 avg score 23.0 time_steps 23 learning_steps 1\n",
            "... saving models ...\n",
            "episode 1 score 37.0 avg score 30.0 time_steps 60 learning_steps 3\n",
            "... saving models ...\n",
            "episode 2 score 39.0 avg score 33.0 time_steps 99 learning_steps 4\n",
            "episode 3 score 26.0 avg score 31.2 time_steps 125 learning_steps 6\n",
            "episode 4 score 15.0 avg score 28.0 time_steps 140 learning_steps 7\n",
            "episode 5 score 13.0 avg score 25.5 time_steps 153 learning_steps 7\n",
            "episode 6 score 30.0 avg score 26.1 time_steps 183 learning_steps 9\n",
            "episode 7 score 22.0 avg score 25.6 time_steps 205 learning_steps 10\n",
            "episode 8 score 40.0 avg score 27.2 time_steps 245 learning_steps 12\n",
            "episode 9 score 16.0 avg score 26.1 time_steps 261 learning_steps 13\n",
            "episode 10 score 23.0 avg score 25.8 time_steps 284 learning_steps 14\n",
            "episode 11 score 48.0 avg score 27.7 time_steps 332 learning_steps 16\n",
            "episode 12 score 31.0 avg score 27.9 time_steps 363 learning_steps 18\n",
            "episode 13 score 20.0 avg score 27.4 time_steps 383 learning_steps 19\n",
            "episode 14 score 82.0 avg score 31.0 time_steps 465 learning_steps 23\n",
            "episode 15 score 15.0 avg score 30.0 time_steps 480 learning_steps 24\n",
            "episode 16 score 18.0 avg score 29.3 time_steps 498 learning_steps 24\n",
            "episode 17 score 14.0 avg score 28.4 time_steps 512 learning_steps 25\n",
            "episode 18 score 10.0 avg score 27.5 time_steps 522 learning_steps 26\n",
            "episode 19 score 20.0 avg score 27.1 time_steps 542 learning_steps 27\n",
            "episode 20 score 23.0 avg score 26.9 time_steps 565 learning_steps 28\n",
            "episode 21 score 14.0 avg score 26.3 time_steps 579 learning_steps 28\n",
            "episode 22 score 24.0 avg score 26.2 time_steps 603 learning_steps 30\n",
            "episode 23 score 33.0 avg score 26.5 time_steps 636 learning_steps 31\n",
            "episode 24 score 92.0 avg score 29.1 time_steps 728 learning_steps 36\n",
            "episode 25 score 91.0 avg score 31.5 time_steps 819 learning_steps 40\n",
            "episode 26 score 45.0 avg score 32.0 time_steps 864 learning_steps 43\n",
            "episode 27 score 43.0 avg score 32.4 time_steps 907 learning_steps 45\n",
            "... saving models ...\n",
            "episode 28 score 51.0 avg score 33.0 time_steps 958 learning_steps 47\n",
            "... saving models ...\n",
            "episode 29 score 48.0 avg score 33.5 time_steps 1006 learning_steps 50\n",
            "... saving models ...\n",
            "episode 30 score 89.0 avg score 35.3 time_steps 1095 learning_steps 54\n",
            "... saving models ...\n",
            "episode 31 score 78.0 avg score 36.7 time_steps 1173 learning_steps 58\n",
            "... saving models ...\n",
            "episode 32 score 157.0 avg score 40.3 time_steps 1330 learning_steps 66\n",
            "... saving models ...\n",
            "episode 33 score 196.0 avg score 44.9 time_steps 1526 learning_steps 76\n",
            "... saving models ...\n",
            "episode 34 score 58.0 avg score 45.3 time_steps 1584 learning_steps 79\n",
            "... saving models ...\n",
            "episode 35 score 105.0 avg score 46.9 time_steps 1689 learning_steps 84\n",
            "episode 36 score 45.0 avg score 46.9 time_steps 1734 learning_steps 86\n",
            "episode 37 score 25.0 avg score 46.3 time_steps 1759 learning_steps 87\n",
            "... saving models ...\n",
            "episode 38 score 167.0 avg score 49.4 time_steps 1926 learning_steps 96\n",
            "... saving models ...\n",
            "episode 39 score 153.0 avg score 52.0 time_steps 2079 learning_steps 103\n",
            "... saving models ...\n",
            "episode 40 score 119.0 avg score 53.6 time_steps 2198 learning_steps 109\n",
            "... saving models ...\n",
            "episode 41 score 98.0 avg score 54.7 time_steps 2296 learning_steps 114\n",
            "... saving models ...\n",
            "episode 42 score 60.0 avg score 54.8 time_steps 2356 learning_steps 117\n",
            "... saving models ...\n",
            "episode 43 score 92.0 avg score 55.6 time_steps 2448 learning_steps 122\n",
            "... saving models ...\n",
            "episode 44 score 109.0 avg score 56.8 time_steps 2557 learning_steps 127\n",
            "... saving models ...\n",
            "episode 45 score 123.0 avg score 58.3 time_steps 2680 learning_steps 134\n",
            "... saving models ...\n",
            "episode 46 score 146.0 avg score 60.1 time_steps 2826 learning_steps 141\n",
            "... saving models ...\n",
            "episode 47 score 182.0 avg score 62.7 time_steps 3008 learning_steps 150\n",
            "... saving models ...\n",
            "episode 48 score 200.0 avg score 65.5 time_steps 3208 learning_steps 160\n",
            "... saving models ...\n",
            "episode 49 score 144.0 avg score 67.0 time_steps 3352 learning_steps 167\n",
            "... saving models ...\n",
            "episode 50 score 200.0 avg score 69.6 time_steps 3552 learning_steps 177\n",
            "episode 51 score 69.0 avg score 69.6 time_steps 3621 learning_steps 181\n",
            "... saving models ...\n",
            "episode 52 score 106.0 avg score 70.3 time_steps 3727 learning_steps 186\n",
            "... saving models ...\n",
            "episode 53 score 72.0 avg score 70.4 time_steps 3799 learning_steps 189\n",
            "... saving models ...\n",
            "episode 54 score 200.0 avg score 72.7 time_steps 3999 learning_steps 199\n",
            "... saving models ...\n",
            "episode 55 score 173.0 avg score 74.5 time_steps 4172 learning_steps 208\n",
            "... saving models ...\n",
            "episode 56 score 85.0 avg score 74.7 time_steps 4257 learning_steps 212\n",
            "episode 57 score 51.0 avg score 74.3 time_steps 4308 learning_steps 215\n",
            "... saving models ...\n",
            "episode 58 score 131.0 avg score 75.2 time_steps 4439 learning_steps 221\n",
            "... saving models ...\n",
            "episode 59 score 114.0 avg score 75.9 time_steps 4553 learning_steps 227\n",
            "... saving models ...\n",
            "episode 60 score 122.0 avg score 76.6 time_steps 4675 learning_steps 233\n",
            "... saving models ...\n",
            "episode 61 score 104.0 avg score 77.1 time_steps 4779 learning_steps 238\n",
            "... saving models ...\n",
            "episode 62 score 147.0 avg score 78.2 time_steps 4926 learning_steps 246\n",
            "... saving models ...\n",
            "episode 63 score 145.0 avg score 79.2 time_steps 5071 learning_steps 253\n",
            "... saving models ...\n",
            "episode 64 score 200.0 avg score 81.1 time_steps 5271 learning_steps 263\n",
            "... saving models ...\n",
            "episode 65 score 200.0 avg score 82.9 time_steps 5471 learning_steps 273\n",
            "... saving models ...\n",
            "episode 66 score 200.0 avg score 84.6 time_steps 5671 learning_steps 283\n",
            "... saving models ...\n",
            "episode 67 score 200.0 avg score 86.3 time_steps 5871 learning_steps 293\n",
            "... saving models ...\n",
            "episode 68 score 190.0 avg score 87.8 time_steps 6061 learning_steps 303\n",
            "... saving models ...\n",
            "episode 69 score 133.0 avg score 88.5 time_steps 6194 learning_steps 309\n",
            "... saving models ...\n",
            "episode 70 score 200.0 avg score 90.1 time_steps 6394 learning_steps 319\n",
            "... saving models ...\n",
            "episode 71 score 174.0 avg score 91.2 time_steps 6568 learning_steps 328\n",
            "... saving models ...\n",
            "episode 72 score 153.0 avg score 92.1 time_steps 6721 learning_steps 336\n",
            "... saving models ...\n",
            "episode 73 score 108.0 avg score 92.3 time_steps 6829 learning_steps 341\n",
            "... saving models ...\n",
            "episode 74 score 136.0 avg score 92.9 time_steps 6965 learning_steps 348\n",
            "... saving models ...\n",
            "episode 75 score 135.0 avg score 93.4 time_steps 7100 learning_steps 355\n",
            "... saving models ...\n",
            "episode 76 score 145.0 avg score 94.1 time_steps 7245 learning_steps 362\n",
            "... saving models ...\n",
            "episode 77 score 171.0 avg score 95.1 time_steps 7416 learning_steps 370\n",
            "... saving models ...\n",
            "episode 78 score 145.0 avg score 95.7 time_steps 7561 learning_steps 378\n",
            "... saving models ...\n",
            "episode 79 score 163.0 avg score 96.5 time_steps 7724 learning_steps 386\n",
            "... saving models ...\n",
            "episode 80 score 190.0 avg score 97.7 time_steps 7914 learning_steps 395\n",
            "... saving models ...\n",
            "episode 81 score 200.0 avg score 99.0 time_steps 8114 learning_steps 405\n",
            "... saving models ...\n",
            "episode 82 score 133.0 avg score 99.4 time_steps 8247 learning_steps 412\n",
            "... saving models ...\n",
            "episode 83 score 149.0 avg score 100.0 time_steps 8396 learning_steps 419\n",
            "... saving models ...\n",
            "episode 84 score 200.0 avg score 101.1 time_steps 8596 learning_steps 429\n",
            "... saving models ...\n",
            "episode 85 score 200.0 avg score 102.3 time_steps 8796 learning_steps 439\n",
            "... saving models ...\n",
            "episode 86 score 114.0 avg score 102.4 time_steps 8910 learning_steps 445\n",
            "... saving models ...\n",
            "episode 87 score 134.0 avg score 102.8 time_steps 9044 learning_steps 452\n",
            "... saving models ...\n",
            "episode 88 score 200.0 avg score 103.9 time_steps 9244 learning_steps 462\n",
            "... saving models ...\n",
            "episode 89 score 107.0 avg score 103.9 time_steps 9351 learning_steps 467\n",
            "... saving models ...\n",
            "episode 90 score 200.0 avg score 105.0 time_steps 9551 learning_steps 477\n",
            "... saving models ...\n",
            "episode 91 score 151.0 avg score 105.5 time_steps 9702 learning_steps 485\n",
            "episode 92 score 24.0 avg score 104.6 time_steps 9726 learning_steps 486\n",
            "episode 93 score 20.0 avg score 103.7 time_steps 9746 learning_steps 487\n",
            "episode 94 score 64.0 avg score 103.3 time_steps 9810 learning_steps 490\n",
            "episode 95 score 58.0 avg score 102.8 time_steps 9868 learning_steps 493\n",
            "episode 96 score 200.0 avg score 103.8 time_steps 10068 learning_steps 503\n",
            "episode 97 score 63.0 avg score 103.4 time_steps 10131 learning_steps 506\n",
            "episode 98 score 95.0 avg score 103.3 time_steps 10226 learning_steps 511\n",
            "episode 99 score 33.0 avg score 102.6 time_steps 10259 learning_steps 512\n",
            "episode 100 score 35.0 avg score 102.7 time_steps 10294 learning_steps 514\n",
            "episode 101 score 58.0 avg score 102.9 time_steps 10352 learning_steps 517\n",
            "episode 102 score 149.0 avg score 104.0 time_steps 10501 learning_steps 525\n",
            "episode 103 score 106.0 avg score 104.8 time_steps 10607 learning_steps 530\n",
            "... saving models ...\n",
            "episode 104 score 143.0 avg score 106.1 time_steps 10750 learning_steps 537\n",
            "... saving models ...\n",
            "episode 105 score 137.0 avg score 107.3 time_steps 10887 learning_steps 544\n",
            "... saving models ...\n",
            "episode 106 score 200.0 avg score 109.0 time_steps 11087 learning_steps 554\n",
            "... saving models ...\n",
            "episode 107 score 146.0 avg score 110.3 time_steps 11233 learning_steps 561\n",
            "... saving models ...\n",
            "episode 108 score 125.0 avg score 111.1 time_steps 11358 learning_steps 567\n",
            "... saving models ...\n",
            "episode 109 score 119.0 avg score 112.2 time_steps 11477 learning_steps 573\n",
            "... saving models ...\n",
            "episode 110 score 159.0 avg score 113.5 time_steps 11636 learning_steps 581\n",
            "... saving models ...\n",
            "episode 111 score 200.0 avg score 115.0 time_steps 11836 learning_steps 591\n",
            "... saving models ...\n",
            "episode 112 score 166.0 avg score 116.4 time_steps 12002 learning_steps 600\n",
            "... saving models ...\n",
            "episode 113 score 199.0 avg score 118.2 time_steps 12201 learning_steps 610\n",
            "... saving models ...\n",
            "episode 114 score 132.0 avg score 118.7 time_steps 12333 learning_steps 616\n",
            "... saving models ...\n",
            "episode 115 score 92.0 avg score 119.5 time_steps 12425 learning_steps 621\n",
            "... saving models ...\n",
            "episode 116 score 81.0 avg score 120.1 time_steps 12506 learning_steps 625\n",
            "... saving models ...\n",
            "episode 117 score 36.0 avg score 120.3 time_steps 12542 learning_steps 627\n",
            "... saving models ...\n",
            "episode 118 score 23.0 avg score 120.4 time_steps 12565 learning_steps 628\n",
            "episode 119 score 16.0 avg score 120.4 time_steps 12581 learning_steps 629\n",
            "episode 120 score 14.0 avg score 120.3 time_steps 12595 learning_steps 629\n",
            "episode 121 score 15.0 avg score 120.3 time_steps 12610 learning_steps 630\n",
            "episode 122 score 18.0 avg score 120.2 time_steps 12628 learning_steps 631\n",
            "episode 123 score 24.0 avg score 120.2 time_steps 12652 learning_steps 632\n",
            "episode 124 score 16.0 avg score 119.4 time_steps 12668 learning_steps 633\n",
            "episode 125 score 11.0 avg score 118.6 time_steps 12679 learning_steps 633\n",
            "episode 126 score 13.0 avg score 118.3 time_steps 12692 learning_steps 634\n",
            "episode 127 score 11.0 avg score 118.0 time_steps 12703 learning_steps 635\n",
            "episode 128 score 11.0 avg score 117.6 time_steps 12714 learning_steps 635\n",
            "episode 129 score 14.0 avg score 117.2 time_steps 12728 learning_steps 636\n",
            "episode 130 score 10.0 avg score 116.4 time_steps 12738 learning_steps 636\n",
            "episode 131 score 44.0 avg score 116.1 time_steps 12782 learning_steps 639\n",
            "episode 132 score 23.0 avg score 114.8 time_steps 12805 learning_steps 640\n",
            "episode 133 score 23.0 avg score 113.0 time_steps 12828 learning_steps 641\n",
            "episode 134 score 73.0 avg score 113.2 time_steps 12901 learning_steps 645\n",
            "episode 135 score 111.0 avg score 113.2 time_steps 13012 learning_steps 650\n",
            "episode 136 score 200.0 avg score 114.8 time_steps 13212 learning_steps 660\n",
            "episode 137 score 181.0 avg score 116.3 time_steps 13393 learning_steps 669\n",
            "episode 138 score 169.0 avg score 116.4 time_steps 13562 learning_steps 678\n",
            "episode 139 score 132.0 avg score 116.2 time_steps 13694 learning_steps 684\n",
            "episode 140 score 200.0 avg score 117.0 time_steps 13894 learning_steps 694\n",
            "episode 141 score 200.0 avg score 118.0 time_steps 14094 learning_steps 704\n",
            "episode 142 score 142.0 avg score 118.8 time_steps 14236 learning_steps 711\n",
            "episode 143 score 173.0 avg score 119.6 time_steps 14409 learning_steps 720\n",
            "... saving models ...\n",
            "episode 144 score 200.0 avg score 120.5 time_steps 14609 learning_steps 730\n",
            "... saving models ...\n",
            "episode 145 score 200.0 avg score 121.3 time_steps 14809 learning_steps 740\n",
            "... saving models ...\n",
            "episode 146 score 200.0 avg score 121.8 time_steps 15009 learning_steps 750\n",
            "... saving models ...\n",
            "episode 147 score 200.0 avg score 122.0 time_steps 15209 learning_steps 760\n",
            "episode 148 score 200.0 avg score 122.0 time_steps 15409 learning_steps 770\n",
            "... saving models ...\n",
            "episode 149 score 200.0 avg score 122.6 time_steps 15609 learning_steps 780\n",
            "episode 150 score 73.0 avg score 121.3 time_steps 15682 learning_steps 784\n",
            "... saving models ...\n",
            "episode 151 score 200.0 avg score 122.6 time_steps 15882 learning_steps 794\n",
            "episode 152 score 80.0 avg score 122.3 time_steps 15962 learning_steps 798\n",
            "episode 153 score 92.0 avg score 122.5 time_steps 16054 learning_steps 802\n",
            "episode 154 score 41.0 avg score 121.0 time_steps 16095 learning_steps 804\n",
            "episode 155 score 25.0 avg score 119.5 time_steps 16120 learning_steps 806\n",
            "episode 156 score 94.0 avg score 119.6 time_steps 16214 learning_steps 810\n",
            "episode 157 score 200.0 avg score 121.1 time_steps 16414 learning_steps 820\n",
            "episode 158 score 200.0 avg score 121.8 time_steps 16614 learning_steps 830\n",
            "episode 159 score 200.0 avg score 122.6 time_steps 16814 learning_steps 840\n",
            "... saving models ...\n",
            "episode 160 score 200.0 avg score 123.4 time_steps 17014 learning_steps 850\n",
            "... saving models ...\n",
            "episode 161 score 200.0 avg score 124.3 time_steps 17214 learning_steps 860\n",
            "... saving models ...\n",
            "episode 162 score 200.0 avg score 124.9 time_steps 17414 learning_steps 870\n",
            "... saving models ...\n",
            "episode 163 score 200.0 avg score 125.4 time_steps 17614 learning_steps 880\n",
            "episode 164 score 200.0 avg score 125.4 time_steps 17814 learning_steps 890\n",
            "episode 165 score 200.0 avg score 125.4 time_steps 18014 learning_steps 900\n",
            "episode 166 score 200.0 avg score 125.4 time_steps 18214 learning_steps 910\n",
            "episode 167 score 200.0 avg score 125.4 time_steps 18414 learning_steps 920\n",
            "... saving models ...\n",
            "episode 168 score 200.0 avg score 125.5 time_steps 18614 learning_steps 930\n",
            "episode 169 score 59.0 avg score 124.8 time_steps 18673 learning_steps 933\n",
            "episode 170 score 186.0 avg score 124.7 time_steps 18859 learning_steps 942\n",
            "episode 171 score 182.0 avg score 124.7 time_steps 19041 learning_steps 952\n",
            "episode 172 score 200.0 avg score 125.2 time_steps 19241 learning_steps 962\n",
            "... saving models ...\n",
            "episode 173 score 200.0 avg score 126.1 time_steps 19441 learning_steps 972\n",
            "... saving models ...\n",
            "episode 174 score 200.0 avg score 126.8 time_steps 19641 learning_steps 982\n",
            "... saving models ...\n",
            "episode 175 score 200.0 avg score 127.4 time_steps 19841 learning_steps 992\n",
            "... saving models ...\n",
            "episode 176 score 200.0 avg score 128.0 time_steps 20041 learning_steps 1002\n",
            "... saving models ...\n",
            "episode 177 score 200.0 avg score 128.2 time_steps 20241 learning_steps 1012\n",
            "... saving models ...\n",
            "episode 178 score 200.0 avg score 128.8 time_steps 20441 learning_steps 1022\n",
            "... saving models ...\n",
            "episode 179 score 200.0 avg score 129.2 time_steps 20641 learning_steps 1032\n",
            "... saving models ...\n",
            "episode 180 score 200.0 avg score 129.3 time_steps 20841 learning_steps 1042\n",
            "episode 181 score 200.0 avg score 129.3 time_steps 21041 learning_steps 1052\n",
            "... saving models ...\n",
            "episode 182 score 200.0 avg score 129.9 time_steps 21241 learning_steps 1062\n",
            "... saving models ...\n",
            "episode 183 score 200.0 avg score 130.4 time_steps 21441 learning_steps 1072\n",
            "episode 184 score 200.0 avg score 130.4 time_steps 21641 learning_steps 1082\n",
            "episode 185 score 200.0 avg score 130.4 time_steps 21841 learning_steps 1092\n",
            "... saving models ...\n",
            "episode 186 score 200.0 avg score 131.3 time_steps 22041 learning_steps 1102\n",
            "... saving models ...\n",
            "episode 187 score 200.0 avg score 132.0 time_steps 22241 learning_steps 1112\n",
            "episode 188 score 200.0 avg score 132.0 time_steps 22441 learning_steps 1122\n",
            "... saving models ...\n",
            "episode 189 score 200.0 avg score 132.9 time_steps 22641 learning_steps 1132\n",
            "episode 190 score 200.0 avg score 132.9 time_steps 22841 learning_steps 1142\n",
            "... saving models ...\n",
            "episode 191 score 200.0 avg score 133.4 time_steps 23041 learning_steps 1152\n",
            "... saving models ...\n",
            "episode 192 score 200.0 avg score 135.2 time_steps 23241 learning_steps 1162\n",
            "... saving models ...\n",
            "episode 193 score 200.0 avg score 136.9 time_steps 23441 learning_steps 1172\n",
            "... saving models ...\n",
            "episode 194 score 200.0 avg score 138.3 time_steps 23641 learning_steps 1182\n",
            "... saving models ...\n",
            "episode 195 score 200.0 avg score 139.7 time_steps 23841 learning_steps 1192\n",
            "episode 196 score 200.0 avg score 139.7 time_steps 24041 learning_steps 1202\n",
            "... saving models ...\n",
            "episode 197 score 200.0 avg score 141.1 time_steps 24241 learning_steps 1212\n",
            "... saving models ...\n",
            "episode 198 score 200.0 avg score 142.2 time_steps 24441 learning_steps 1222\n",
            "... saving models ...\n",
            "episode 199 score 200.0 avg score 143.8 time_steps 24641 learning_steps 1232\n",
            "... saving models ...\n",
            "episode 200 score 200.0 avg score 145.5 time_steps 24841 learning_steps 1242\n",
            "... saving models ...\n",
            "episode 201 score 200.0 avg score 146.9 time_steps 25041 learning_steps 1252\n",
            "... saving models ...\n",
            "episode 202 score 200.0 avg score 147.4 time_steps 25241 learning_steps 1262\n",
            "... saving models ...\n",
            "episode 203 score 200.0 avg score 148.3 time_steps 25441 learning_steps 1272\n",
            "... saving models ...\n",
            "episode 204 score 200.0 avg score 148.9 time_steps 25641 learning_steps 1282\n",
            "... saving models ...\n",
            "episode 205 score 200.0 avg score 149.5 time_steps 25841 learning_steps 1292\n",
            "episode 206 score 200.0 avg score 149.5 time_steps 26041 learning_steps 1302\n",
            "... saving models ...\n",
            "episode 207 score 200.0 avg score 150.1 time_steps 26241 learning_steps 1312\n",
            "... saving models ...\n",
            "episode 208 score 200.0 avg score 150.8 time_steps 26441 learning_steps 1322\n",
            "... saving models ...\n",
            "episode 209 score 200.0 avg score 151.6 time_steps 26641 learning_steps 1332\n",
            "... saving models ...\n",
            "episode 210 score 200.0 avg score 152.1 time_steps 26841 learning_steps 1342\n",
            "episode 211 score 200.0 avg score 152.1 time_steps 27041 learning_steps 1352\n",
            "... saving models ...\n",
            "episode 212 score 200.0 avg score 152.4 time_steps 27241 learning_steps 1362\n",
            "... saving models ...\n",
            "episode 213 score 200.0 avg score 152.4 time_steps 27441 learning_steps 1372\n",
            "... saving models ...\n",
            "episode 214 score 200.0 avg score 153.1 time_steps 27641 learning_steps 1382\n",
            "... saving models ...\n",
            "episode 215 score 200.0 avg score 154.2 time_steps 27841 learning_steps 1392\n",
            "... saving models ...\n",
            "episode 216 score 200.0 avg score 155.3 time_steps 28041 learning_steps 1402\n",
            "... saving models ...\n",
            "episode 217 score 200.0 avg score 157.0 time_steps 28241 learning_steps 1412\n",
            "... saving models ...\n",
            "episode 218 score 200.0 avg score 158.8 time_steps 28441 learning_steps 1422\n",
            "... saving models ...\n",
            "episode 219 score 200.0 avg score 160.6 time_steps 28641 learning_steps 1432\n",
            "... saving models ...\n",
            "episode 220 score 200.0 avg score 162.5 time_steps 28841 learning_steps 1442\n",
            "... saving models ...\n",
            "episode 221 score 200.0 avg score 164.3 time_steps 29041 learning_steps 1452\n",
            "... saving models ...\n",
            "episode 222 score 200.0 avg score 166.1 time_steps 29241 learning_steps 1462\n",
            "... saving models ...\n",
            "episode 223 score 200.0 avg score 167.9 time_steps 29441 learning_steps 1472\n",
            "... saving models ...\n",
            "episode 224 score 200.0 avg score 169.7 time_steps 29641 learning_steps 1482\n",
            "... saving models ...\n",
            "episode 225 score 200.0 avg score 171.6 time_steps 29841 learning_steps 1492\n",
            "... saving models ...\n",
            "episode 226 score 200.0 avg score 173.5 time_steps 30041 learning_steps 1502\n",
            "... saving models ...\n",
            "episode 227 score 200.0 avg score 175.4 time_steps 30241 learning_steps 1512\n",
            "... saving models ...\n",
            "episode 228 score 200.0 avg score 177.3 time_steps 30441 learning_steps 1522\n",
            "... saving models ...\n",
            "episode 229 score 200.0 avg score 179.1 time_steps 30641 learning_steps 1532\n",
            "... saving models ...\n",
            "episode 230 score 200.0 avg score 181.0 time_steps 30841 learning_steps 1542\n",
            "... saving models ...\n",
            "episode 231 score 200.0 avg score 182.6 time_steps 31041 learning_steps 1552\n",
            "... saving models ...\n",
            "episode 232 score 200.0 avg score 184.4 time_steps 31241 learning_steps 1562\n",
            "... saving models ...\n",
            "episode 233 score 55.0 avg score 184.7 time_steps 31296 learning_steps 1564\n",
            "... saving models ...\n",
            "episode 234 score 200.0 avg score 185.9 time_steps 31496 learning_steps 1574\n",
            "... saving models ...\n",
            "episode 235 score 200.0 avg score 186.8 time_steps 31696 learning_steps 1584\n",
            "episode 236 score 200.0 avg score 186.8 time_steps 31896 learning_steps 1594\n",
            "... saving models ...\n",
            "episode 237 score 200.0 avg score 187.0 time_steps 32096 learning_steps 1604\n",
            "... saving models ...\n",
            "episode 238 score 200.0 avg score 187.3 time_steps 32296 learning_steps 1614\n",
            "... saving models ...\n",
            "episode 239 score 200.0 avg score 188.0 time_steps 32496 learning_steps 1624\n",
            "episode 240 score 200.0 avg score 188.0 time_steps 32696 learning_steps 1634\n",
            "episode 241 score 200.0 avg score 188.0 time_steps 32896 learning_steps 1644\n",
            "... saving models ...\n",
            "episode 242 score 200.0 avg score 188.6 time_steps 33096 learning_steps 1654\n",
            "... saving models ...\n",
            "episode 243 score 200.0 avg score 188.9 time_steps 33296 learning_steps 1664\n",
            "episode 244 score 200.0 avg score 188.9 time_steps 33496 learning_steps 1674\n",
            "episode 245 score 200.0 avg score 188.9 time_steps 33696 learning_steps 1684\n",
            "episode 246 score 200.0 avg score 188.9 time_steps 33896 learning_steps 1694\n",
            "episode 247 score 200.0 avg score 188.9 time_steps 34096 learning_steps 1704\n",
            "episode 248 score 200.0 avg score 188.9 time_steps 34296 learning_steps 1714\n",
            "episode 249 score 200.0 avg score 188.9 time_steps 34496 learning_steps 1724\n",
            "... saving models ...\n",
            "episode 250 score 200.0 avg score 190.1 time_steps 34696 learning_steps 1734\n",
            "episode 251 score 200.0 avg score 190.1 time_steps 34896 learning_steps 1744\n",
            "... saving models ...\n",
            "episode 252 score 200.0 avg score 191.3 time_steps 35096 learning_steps 1754\n",
            "... saving models ...\n",
            "episode 253 score 200.0 avg score 192.4 time_steps 35296 learning_steps 1764\n",
            "... saving models ...\n",
            "episode 254 score 200.0 avg score 194.0 time_steps 35496 learning_steps 1774\n",
            "... saving models ...\n",
            "episode 255 score 200.0 avg score 195.8 time_steps 35696 learning_steps 1784\n",
            "... saving models ...\n",
            "episode 256 score 200.0 avg score 196.8 time_steps 35896 learning_steps 1794\n",
            "episode 257 score 200.0 avg score 196.8 time_steps 36096 learning_steps 1804\n",
            "episode 258 score 200.0 avg score 196.8 time_steps 36296 learning_steps 1814\n",
            "episode 259 score 200.0 avg score 196.8 time_steps 36496 learning_steps 1824\n",
            "episode 260 score 200.0 avg score 196.8 time_steps 36696 learning_steps 1834\n",
            "episode 261 score 200.0 avg score 196.8 time_steps 36896 learning_steps 1844\n",
            "episode 262 score 200.0 avg score 196.8 time_steps 37096 learning_steps 1854\n",
            "episode 263 score 200.0 avg score 196.8 time_steps 37296 learning_steps 1864\n",
            "episode 264 score 200.0 avg score 196.8 time_steps 37496 learning_steps 1874\n",
            "episode 265 score 200.0 avg score 196.8 time_steps 37696 learning_steps 1884\n",
            "episode 266 score 200.0 avg score 196.8 time_steps 37896 learning_steps 1894\n",
            "episode 267 score 200.0 avg score 196.8 time_steps 38096 learning_steps 1904\n",
            "episode 268 score 200.0 avg score 196.8 time_steps 38296 learning_steps 1914\n",
            "... saving models ...\n",
            "episode 269 score 200.0 avg score 198.2 time_steps 38496 learning_steps 1924\n",
            "... saving models ...\n",
            "episode 270 score 200.0 avg score 198.4 time_steps 38696 learning_steps 1934\n",
            "... saving models ...\n",
            "episode 271 score 200.0 avg score 198.6 time_steps 38896 learning_steps 1944\n",
            "episode 272 score 200.0 avg score 198.6 time_steps 39096 learning_steps 1954\n",
            "episode 273 score 200.0 avg score 198.6 time_steps 39296 learning_steps 1964\n",
            "episode 274 score 200.0 avg score 198.6 time_steps 39496 learning_steps 1974\n",
            "episode 275 score 200.0 avg score 198.6 time_steps 39696 learning_steps 1984\n",
            "episode 276 score 200.0 avg score 198.6 time_steps 39896 learning_steps 1994\n",
            "episode 277 score 200.0 avg score 198.6 time_steps 40096 learning_steps 2004\n",
            "episode 278 score 200.0 avg score 198.6 time_steps 40296 learning_steps 2014\n",
            "episode 279 score 200.0 avg score 198.6 time_steps 40496 learning_steps 2024\n",
            "episode 280 score 200.0 avg score 198.6 time_steps 40696 learning_steps 2034\n",
            "episode 281 score 200.0 avg score 198.6 time_steps 40896 learning_steps 2044\n",
            "episode 282 score 200.0 avg score 198.6 time_steps 41096 learning_steps 2054\n",
            "episode 283 score 200.0 avg score 198.6 time_steps 41296 learning_steps 2064\n",
            "episode 284 score 200.0 avg score 198.6 time_steps 41496 learning_steps 2074\n",
            "episode 285 score 200.0 avg score 198.6 time_steps 41696 learning_steps 2084\n",
            "episode 286 score 200.0 avg score 198.6 time_steps 41896 learning_steps 2094\n",
            "episode 287 score 200.0 avg score 198.6 time_steps 42096 learning_steps 2104\n",
            "episode 288 score 200.0 avg score 198.6 time_steps 42296 learning_steps 2114\n",
            "episode 289 score 200.0 avg score 198.6 time_steps 42496 learning_steps 2124\n",
            "episode 290 score 200.0 avg score 198.6 time_steps 42696 learning_steps 2134\n",
            "episode 291 score 200.0 avg score 198.6 time_steps 42896 learning_steps 2144\n",
            "episode 292 score 137.0 avg score 197.9 time_steps 43033 learning_steps 2151\n",
            "episode 293 score 200.0 avg score 197.9 time_steps 43233 learning_steps 2161\n",
            "episode 294 score 200.0 avg score 197.9 time_steps 43433 learning_steps 2171\n",
            "episode 295 score 200.0 avg score 197.9 time_steps 43633 learning_steps 2181\n",
            "episode 296 score 200.0 avg score 197.9 time_steps 43833 learning_steps 2191\n",
            "episode 297 score 200.0 avg score 197.9 time_steps 44033 learning_steps 2201\n",
            "episode 298 score 200.0 avg score 197.9 time_steps 44233 learning_steps 2211\n",
            "episode 299 score 200.0 avg score 197.9 time_steps 44433 learning_steps 2221\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}